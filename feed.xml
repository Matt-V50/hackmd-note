<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://ucas.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ucas.io/" rel="alternate" type="text/html" /><updated>2025-03-13T00:54:55+08:00</updated><id>https://ucas.io/feed.xml</id><title type="html">Notes B</title><subtitle>A place for notes on various topics.</subtitle><author><name>麦丽素</name></author><entry><title type="html">FruitNerf</title><link href="https://ucas.io/ai/3d/fruit-nerf/" rel="alternate" type="text/html" title="FruitNerf" /><published>2024-09-28T23:00:17+08:00</published><updated>2024-09-28T23:00:17+08:00</updated><id>https://ucas.io/ai/3d/fruit-nerf</id><content type="html" xml:base="https://ucas.io/ai/3d/fruit-nerf/">&lt;h2 id=&quot;fruitnerf-a-unified-neural-radiance-field-based--fruit-counting-framework&quot;&gt;FruitNeRF: A Unified Neural Radiance Field based  Fruit Counting Framework&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240928230100.png&quot; alt=&quot;20240928230100&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The main idea of this paper is to use segmentation result from SAM or GroundingDINO to generate another neural radiance field to point out the fruit in 3d space. Then use the high light point to count fruit with point clusting algorithm.&lt;/li&gt;
  &lt;li&gt;Therefore, this paper highly rely on the result from former Segment model. To do that, this author fine-tune this model and compare with the original model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2dgs-2d-gaussian-splatting-for-geometrically-accurate-radiance-fields&quot;&gt;2DGS: 2D Gaussian Splatting for Geometrically Accurate Radiance Fields&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240928230534.png&quot; alt=&quot;20240928230534&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This paper focus on fix the 3d spacial consistency problem in 3DGS. The author believe that, that issue is caused the 3DGS rendering process. The inconsistency of 3D GS sphere when the noval view change is uncontious.&lt;/li&gt;
  &lt;li&gt;Therefore, this author try to use the 2D GS Disk to restain the rendering result from different view. Basically, 2DGS using the surfal method to replace the 3D volume rendering method to get a better result.&lt;/li&gt;
  &lt;li&gt;However, unlike 3DGS, 2DGS is not very good at reconstructing on transparent object, such like, water and glass.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;citygaussian-real-time-high-quality-large-scale--scene-rendering-with-gaussians&quot;&gt;CityGaussian: Real-time High-quality Large-Scale  Scene Rendering with Gaussians&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240928231527.png&quot; alt=&quot;20240928231527&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The main strategy of this paper is to split each tile into a small space block to speed up the rendering and training process.&lt;/li&gt;
  &lt;li&gt;There are two core contributions of this paper. First, the author use divide-and-conquer strategy and block-wise strategy to split the large-scale scene, which usually in urban reconstruction task, into several small block. For rendering process, the author use a method like volume rendering to calculate the relationship of each block from far to near from the camera.&lt;/li&gt;
  &lt;li&gt;Second, the author try to apply a floater removing algorithm to remove the foggy artifact, which usually appear in the 3DGS result. This method is more like a statistical algorithm, called Median Absolute Deviation, I’m not very familiar with this method, not sure why the author chose this to do that.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;grounding-dino-marrying-dino-with-grounded--pre-training-for-open-set-object-detection&quot;&gt;Grounding DINO: Marrying DINO with Grounded  Pre-Training for Open-Set Object Detection&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240928233620.png&quot; alt=&quot;20240928233620&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For segmentation task, the traditional method is more for the close dataset, which means that, such method can not handle all kind of un-seen data, which is the Open-Vocabulary task.&lt;/li&gt;
  &lt;li&gt;To do that, typically, there are two general way to solve this problem. Referring and Grounding. The former one is based on CLIP method and compare the contrastive loss between the clip predict result and the ground-truth. The latter one is based on the BERT method to minimize the contrastive loss. However, unlike the Referring method, the Grounding method put a sequence of word for a single one picture to predict the result bbox and the label.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;grounded-sam-assembling-open-world-models-for-diverse-visual-tasks&quot;&gt;Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240928233542.png&quot; alt=&quot;20240928233542&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The key idea is to combine SAM and GroundingDINO to generate a more accurate result for the fruit counting task.&lt;/li&gt;
  &lt;li&gt;Grounded: Object Detection task&lt;/li&gt;
  &lt;li&gt;SAM: Semantic Segmentation task&lt;/li&gt;
  &lt;li&gt;The main pipeline is. First, use Grounded model to generate the bbox. Then feed this bbox as a prompt to SAM model to get a more accurate segmentation result.&lt;/li&gt;
  &lt;li&gt;For auto-annotation task, we could use RAM-Grounded-SAM.&lt;/li&gt;
  &lt;li&gt;For high controllable image editing task, we could use Grounded-SAM-SD&lt;/li&gt;
  &lt;li&gt;For Promptable Human Motion Analysis, we could use Grounded-SAM-OSX&lt;/li&gt;
  &lt;li&gt;To decompose task to makes the reasoning process more explainable.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework</summary></entry><entry><title type="html">Nerf and 3DGS</title><link href="https://ucas.io/ai/3d/nerf-and-3dgs/" rel="alternate" type="text/html" title="Nerf and 3DGS" /><published>2024-08-10T09:22:57+08:00</published><updated>2024-08-10T09:22:57+08:00</updated><id>https://ucas.io/ai/3d/nerf-and-3dgs</id><content type="html" xml:base="https://ucas.io/ai/3d/nerf-and-3dgs/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240701120650.png&quot; alt=&quot;20240701120650&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;trans-lora-towards-data-free-transferable-parameter-efficient-finetuning&quot;&gt;Trans-LoRA: towards data-free Transferable Parameter Efficient Finetuning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240810092532.png&quot; alt=&quot;20240810092532&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The main contributions of this method are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;It use a Discriminator module to fit the distribution of task data to reduce data transfer.&lt;/li&gt;
  &lt;li&gt;This training process of lora for new base model is simplified.
Problems&lt;/li&gt;
  &lt;li&gt;This method mainly focus on the transfer between Llama and Gemma. No other base model be compared.&lt;/li&gt;
  &lt;li&gt;Not sure how long will be spent by using discriminator module to generate synthetic data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis&quot;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;First of all, this paper using 5 different parameters to describe the image pixel, which could be consider as a light ray.&lt;/li&gt;
  &lt;li&gt;And then, sample a series of points from the ray. Therefore, we could use x,y,z and theta, phi to describe the specific color from different direction, which is a kind of voxel fog.&lt;/li&gt;
  &lt;li&gt;Through an implicit function, the output should be the color and the density.&lt;/li&gt;
  &lt;li&gt;There is a very interesting point. The author split sigma from the last two step of network, and concat back to the network. The explanation is, the author believe that, the voxel fog should have the same density, but different color from different direction.&lt;/li&gt;
  &lt;li&gt;Second of all, this paper out put the color and density to prepare for volume rendering, which as described in equation 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240810092754.png&quot; alt=&quot;20240810092754&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For enhance the result quality, nerf using stratified[死dry ty fy] sampling to implement that.&lt;/li&gt;
  &lt;li&gt;First, use 64 uniform sampling point to describe the ray curve.
And then, use another 128 point to get high frequency information&lt;/li&gt;
  &lt;li&gt;Here the fine sample point is actually from the pdf function. We could use softmax to smooth weight parameter, then get the cdf function.&lt;/li&gt;
  &lt;li&gt;The last detail is positional encoding.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240810092810.png&quot; alt=&quot;20240810092810&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3d-gaussian-splatting-for-real-time-radiance-field-rendering&quot;&gt;3D Gaussian Splatting for Real-Time Radiance Field Rendering&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240810093310.png&quot; alt=&quot;20240810093310&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, GS initial the gs spherial point cloud with three cloest point by calculated with knn.&lt;/li&gt;
  &lt;li&gt;Second, GS use some camera matrix and differentiable tile rasterization to get the 2d image.&lt;/li&gt;
  &lt;li&gt;Third, GS use adaptive density contorl to clone or splat one or more gs sphere to fit the result.&lt;/li&gt;
  &lt;li&gt;The loss is just MSE.&lt;/li&gt;
  &lt;li&gt;The main contribution of this paper is, it use lots of cuda to speed up the computating process. It also implement a cuda method to rasterizer the 3dgs cloud point.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;atomgs-atomizing-gaussian-splatting-for-high-fidelity-radiance-field&quot;&gt;AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240810093935.png&quot; alt=&quot;20240810093935&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The first point of optimization is way of Initialization.&lt;/li&gt;
  &lt;li&gt;Not like GS, AtomGS use isotropic spheroids with a uniform size instead of anisotropic spheroids with of varying sizes.&lt;/li&gt;
  &lt;li&gt;The author believe this will impose the priority of densifying them to accurately fill gap in the geometry.&lt;/li&gt;
  &lt;li&gt;The second optimization is that introduce Normal Loss and Multi-scale ssim loss to control more detail.&lt;/li&gt;
  &lt;li&gt;Here Normal Loss is calculated by multiplying curvature map and edge map with a weight function w(omega).&lt;/li&gt;
  &lt;li&gt;The adptive density controlling has a little bit difference, AtomGS set a new threshold tau_c and tau_s to control the clone and split process.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nerfbusters-removing-ghostly-artifacts-from-casually-captured-nerfs&quot;&gt;Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240810103008.png&quot; alt=&quot;20240810103008&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a local 3d diffusion prior and a density socre distillation sample loss.&lt;/li&gt;
  &lt;li&gt;use importance sampling to query cube with nerf densities.&lt;/li&gt;
  &lt;li&gt;Computing a density score distillation sampling(DSDS) that penalize Nerf diffusion model.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html"></summary></entry><entry><title type="html">AnySyn3D Paper sets</title><link href="https://ucas.io/ai/3d/AnySyn3D-Paper-sets/" rel="alternate" type="text/html" title="AnySyn3D Paper sets" /><published>2024-06-30T00:11:40+08:00</published><updated>2024-06-30T00:11:40+08:00</updated><id>https://ucas.io/ai/3d/AnySyn3D-Paper-sets</id><content type="html" xml:base="https://ucas.io/ai/3d/AnySyn3D-Paper-sets/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240701120650.png&quot; alt=&quot;20240701120650&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;clay-a-controllable-large-scale-generative-model-for-creating-high-quality-3d-assets&quot;&gt;CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;主要贡献
    &lt;ul&gt;
      &lt;li&gt;CLAY的核心是一个在大规模3D数据上预训练的1.5亿参数的生成模型。该模型使用多分辨率VAE对几何形状进行编码,并用一个极简的latent diffusion transformer进行几何生成。
        &lt;ul&gt;
          &lt;li&gt;将多分辨率VAE和极简的latent diffusion transformer结合,并在1.5亿参数量级上实现了模型的高效训练&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;为了训练该大规模模型,作者提出了一个数据标准化流程,包括重新网格化来统一不同来源的3D数据格式,以及利用GPT-4V对数据自动标注。
        &lt;ul&gt;
          &lt;li&gt;统一重新网格化和利用GPT-4V进行自动语义标注&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;在几何生成的基础上,CLAY还可以合成PBR材质贴图,实现逼真的渲染效果。作者训练了一个多视角材质扩散模型来高效地生成高分辨率的漫反射、粗糙度等贴图。&lt;/li&gt;
      &lt;li&gt;CLAY支持低秩适应(LoRA)微调以及多种条件生成,可以根据草图、体素、包围盒、点云等多种输入控制生成。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sketchdream-sketch-based-text-to-3d-generation-and-editing&quot;&gt;SketchDream: Sketch-based Text-to-3D Generation and Editing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;同时使用单视角手绘草图和文本提示作为输入,生成高质量3D模型的端到端系统。生成的结果既能忠实于输入的草图,也能满足文本的描述。&lt;/li&gt;
  &lt;li&gt;利用深度引导的扭曲策略来建立视角间的空间对应关系,并使用3D注意力模块来确保不同视角下生成图像的3D一致性&lt;/li&gt;
  &lt;li&gt;粗到精的两阶段编辑框架&lt;/li&gt;
  &lt;li&gt;支持对重建或生成的NeRF进行局部细节编辑&lt;/li&gt;
  &lt;li&gt;并采用3D注意力控制模块确保3D一致性&lt;/li&gt;
  &lt;li&gt;对于新视角图像,最近视角的输入是通过深度扭曲得到的草图,其他视角的输入是空白图像。为了确保不同视角下生成图像的3D一致性,该模块采用了MVDream中的3D注意力机制&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;direct3d-scalable-image-to-3d-generation-via-3d-latent-diffusion-transformer&quot;&gt;Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240701122302.png&quot; alt=&quot;20240701122302&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提出了D3D-VAE模型,可以将高分辨率的点云编码到一个紧致的3D latent空间中。不同于以往方法使用渲染图像作为监督信号,D3D-VAE直接对解码后的几何形状进行监督,采用了半连续的表面采样策略,更好地保留了3D形状的细节
    &lt;ul&gt;
      &lt;li&gt;在D3D-VAE中,编码器将点云压缩到三平面latent空间,解码器再将低分辨率的latent表示上采样到高分辨率的三平面特征图,最后通过可学习的MLP将三个特征图映射为occupancy场,以重建3D形状。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;提出了D3D-DiT扩散模型,可以根据输入的参考图像在3D latent空间中生成与之一致的3D模型。D3D-DiT专门设计了像素级和语义级两个层面的图像对齐模块,使生成的3D模型能在局部细节和整体语义上与输入图像保持一致。&lt;/li&gt;
  &lt;li&gt;采用了显式的triplane latent表示,比隐式编码更利于3D信息的保留;提出的像素级和语义级图像条件对齐模块有助于提升3D生成的细节和一致性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;blender-gpt-scenecraft-an-llm-agent-for-synthesizing-3d-scene-as-blender-code&quot;&gt;Blender GPT; SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240701122625.png&quot; alt=&quot;20240701122625&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用一个双循环优化流程:
    &lt;ul&gt;
      &lt;li&gt;内循环针对每个场景进行优化。给定文本查询,LLM规划器构建一个场景图,详细说明场景中资产之间的空间关系。然后SceneCraft基于图编写Python脚本,将关系转化为数值约束条件。渲染图像后,利用GPT-V等多模态LLM分析图像,判断是否与文本描述匹配,如果不匹配则识别出问题所在并迭代优化脚本。&lt;/li&gt;
      &lt;li&gt;外循环则动态扩展SceneCraft的”空间技能”库。它回顾内循环中约束脚本的渐进式变化,识别出其中通用的代码模式并整合到库中,从而实现不依赖昂贵的LLM参数调优的持续自我完善。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SceneCraft只需要在20个带有ground-truth约束的合成查询上进行双循环优化,就能开发出一个鲁棒的技能库。相比传统的模型微调,这种方法更加高效和可适应性强。&lt;/li&gt;
  &lt;li&gt;在合成数据集和真实世界的Sintel电影数据集上的实验表明,SceneCraft能比现有的LLM智能体如BlenderGPT更高效准确地根据文本生成复杂3D场景。定量上,SceneCraft在CLIP相似度、满足约束条件的比例等指标上大幅超越基线;定性上,SceneCraft生成的场景和视频也更准确地捕捉到了文本描述的细节。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unique3d-high-quality-and-efficient-3d-mesh-generation-from-a-single-image&quot;&gt;Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240701123125.png&quot; alt=&quot;20240701123125&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Unique3D能够在30秒内从单视角图像高效生成高质量的3D网格,具有目前最先进的生成保真度和泛化能力。&lt;/li&gt;
  &lt;li&gt;提出了一种多级上采样策略,可以逐步提高生成的多视图图像和法线贴图的分辨率。&lt;/li&gt;
  &lt;li&gt;设计了一种新的即时一致网格重建算法ISOMER(Instant and Consistent Mesh Reconstruction),可以从RGB图像和法线贴图中重建几何细节和纹理丰富的3D网格。
    &lt;ul&gt;
      &lt;li&gt;初始网格估计(Initial Mesh Estimation):直接从正面和背面视图估计目标形状的初始网格拓扑结构。将正面视图的法线贴图积分得到深度图,映射每个像素到空间位置,从而分别从物体的正面和背面视图创建网格模型。通过泊松重建将两个模型无缝连接,并简化成2000个面作为初始网格。&lt;/li&gt;
      &lt;li&gt;从粗到细的网格优化(Coarse-to-Fine Mesh Optimization):通过最小化基于掩码和法线的损失函数,迭代优化网格模型逼近目标形状。每次优化时先进行可微渲染计算损失和梯度,再根据梯度移动顶点,最后通过边坍缩、边分裂和边翻转校正网格。数百次迭代后网格收敛到目标形状的粗略近似。引入了一个膨胀正则化(Expansion Regularization)来避免法线监督下网格塌缩。&lt;/li&gt;
      &lt;li&gt;多视图不一致性的显式优化目标(ExplicitTarget for Multi-view Inconsistency):由于野外图像的多视图不一致性,没有完美匹配所有视角的解。因此提出了一种为每个顶点分配唯一优化目标的方法,更加鲁棒地引导优化方向。ExplicitTarget将可见视图的加权平均作为每个顶点颜色的优化目标,权重由投影面积和法线预测置信度决定。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在单图像到3D任务上进行了大量实验,证明了该方法的有效性和生成保真度,为3D生成AI在现实应用中开辟了新的可能性。&lt;/li&gt;
  &lt;li&gt;方法主要分为3个步骤:
    &lt;ul&gt;
      &lt;li&gt;利用多视图扩散模型从输入图像生成4个正交多视图图像及其法线贴图,并通过多级上采样将其分辨率提升。&lt;/li&gt;
      &lt;li&gt;利用ISOMER算法从高分辨率RGB图像和法线贴图即时重建出高质量一致的3D网格,该算法充分整合了颜色和几何先验。&lt;/li&gt;
      &lt;li&gt;在生成的网格上进行着色,得到最终的带纹理3D模型。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html"></summary></entry><entry><title type="html">Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit Representation for Diverse 3D Shapes</title><link href="https://ucas.io/ai/3d/Unsigned-Orthogonal-Distance-Fields-UODF/" rel="alternate" type="text/html" title="Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit Representation for Diverse 3D Shapes" /><published>2024-05-13T00:11:40+08:00</published><updated>2024-05-13T00:11:40+08:00</updated><id>https://ucas.io/ai/3d/Unsigned-Orthogonal-Distance-Fields-UODF</id><content type="html" xml:base="https://ucas.io/ai/3d/Unsigned-Orthogonal-Distance-Fields-UODF/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240512235923.png&quot; alt=&quot;20240512235923&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;unsigned-orthogonal-distance-fields-an-accurate-neural-implicit-representation-for-diverse-3d-shapes&quot;&gt;Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit Representation for Diverse 3D Shapes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;主要贡献
    &lt;ul&gt;
      &lt;li&gt;UODFs定义为沿三个正交方向(左右、前后、上下)的无符号距离场。每个空间点的UODFs值表示该点沿三个正交方向到形状表面的最短距离。&lt;/li&gt;
      &lt;li&gt;UODFs有三个主要特点:
        &lt;ul&gt;
          &lt;li&gt;由三个正交方向的无符号距离场组成&lt;/li&gt;
          &lt;li&gt;每个点的UODFs在每个正交方向上的一维导数绝对值等于1&lt;/li&gt;
          &lt;li&gt;相邻射线之间的UODFs值可能不连续&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;使用三个MLP神经网络分别拟合三个正交方向的UODFs。对每个射线等间隔采样点,每个采样点可直接估计其最近的表面点。&lt;/li&gt;
      &lt;li&gt;提出了一种从多个采样点估计表面点的方法,可以消除插值误差,只受神经网络拟合误差影响。最后融合三个方向估计的表面点得到最终结果。&lt;/li&gt;
      &lt;li&gt;在各种3D模型(封闭、开放、多层、组装等)上进行实验,UODFs的重建精度显著优于SDF、UDF等其他隐式表示方法,尤其是开放曲面和小尺寸点云重建。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nc-sdf-enhancing-indoor-scene-reconstruction-using-neural-sdfs-with-view-dependent-normal-compensation&quot;&gt;NC-SDF: Enhancing Indoor Scene Reconstruction Using Neural SDFs with View-Dependent Normal Compensation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240513001203.png&quot; alt=&quot;20240513001203&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提出了一种视角相关的法向补偿模型(view-dependent normal compensation model),用于解决单目几何先验在多视角下的不一致性问题。通过将法向先验中视角相关的偏差显式建模到场景的隐式表示中,并自适应地学习和校正这些偏差,可以有效缓解不一致监督带来的负面影响,提高重建的全局一致性和局部细节。
    &lt;ul&gt;
      &lt;li&gt;解决单目几何先验(尤其是法向先验)在多视角下不一致的问题&lt;/li&gt;
      &lt;li&gt;在之前的方法中,研究者通常直接将预测的单目法向图作为监督信号,引导隐式场景表示的优化。然而,由于预测网络是在单视角下进行估计的,所以预测的法向图往往与场景的真实法向不完全一致,并且这种偏差具有视角相关性。当多个视角的法向预测结果存在不一致时,会给隐式表示的优化带来困难,导致重建质量下降。&lt;/li&gt;
      &lt;li&gt;为了解决这个问题,论文提出将每个视角的法向偏差也建模到场景的隐式表示中。具体来说,他们设计了一个法向补偿模型,以视角方向为输入,预测对应视角下的法向补偿旋转角度。通过将这个旋转作用在隐式表示的法向上,就得到了经过补偿的法向。优化时,渲染得到的补偿后法向图与单目法向预测结果对齐,而不是直接用隐式表示的法向。&lt;/li&gt;
      &lt;li&gt;在训练过程中,法向补偿模型与颜色模型和几何模型一起联合优化。这种方式可以自适应地学习和校正不同视角下的法向偏差,使隐式表示更加鲁棒,从而提高重建质量。可视化结果表明,通过法向补偿,最终渲染得到的法向图与颜色图在不同视角下都更加一致,证明该模型能有效建模视角相关的法向偏差。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;设计了一种信息量丰富的像素采样策略,通过优先采样信息量高的像素,使模型更加关注复杂的几何细节。&lt;/li&gt;
  &lt;li&gt;提出了一种基于特征融合的混合几何建模方法,利用MLP的感应平滑性来确保平滑表面,同时利用体素网格提供的高频编码来捕捉精细几何。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;clusteringsdf-self-organized-neural-implicit-surfaces-for-3d-decomposition&quot;&gt;ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240513002321.png&quot; alt=&quot;20240513002321&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提出了一种新颖的方法ClusteringSDF,可以将2D机器生成的分割标签融合到3D中,同时通过神经隐式表面重建(SDF)来重建场景中物体的表面。
    &lt;ul&gt;
      &lt;li&gt;预训练的2D分割模型Mask2Former生成2D分割标签作为弱监督信号&lt;/li&gt;
      &lt;li&gt;用于监督的分割标签来自Mask2Former,而对于定量指标,我们将预测结果与groundtruth标签进行了比较&lt;/li&gt;
      &lt;li&gt;有助于提升3D分割的性能&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;设计了一种高效的聚类机制,用于对齐这些多视图不一致的标签,生成一致的3D分割表示。该方法通过归一化将聚类中心限制在一个单纯形内,从而实现高效聚类。&lt;/li&gt;
  &lt;li&gt;在没有真值标签监督的情况下,ClusteringSDF保持了场景和物体的对象组合表示。&lt;/li&gt;
  &lt;li&gt;实验结果表明,该模型在渲染一致的3D分割标签方面达到了最先进的性能。同时,与最新的基于NeRF的3D分割方法相比,ClusteringSDF的训练时间不到四分之一。
展示了该方法完全从不一致的2D标签重建场景中单个物体表面的能力。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mosaic-sdf-for-3d-generative-models&quot;&gt;Mosaic-SDF for 3D Generative Models&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img1715531534585.png&quot; alt=&quot;1715531534585&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提出了一种新的3D形状表示方法Mosaic-SDF(M-SDF),可以近似任意形状的SDF。M-SDF使用一组局部网格来近似SDF,这些网格分布在形状边界附近。该表示法具有以下优点:&lt;/li&gt;
  &lt;li&gt;预处理效率高:可以快速计算大规模数据集中每个形状的M-SDF表示,计算过程易于并行化。&lt;/li&gt;
  &lt;li&gt;参数效率高:只在形状边界附近覆盖空间,提供了良好的近似能力和参数数量的平衡。&lt;/li&gt;
  &lt;li&gt;简单的张量结构:M-SDF是一个简单的矩阵形式,与Transformer等强大的神经网络架构兼容。&lt;/li&gt;
  &lt;li&gt;作者利用M-SDF训练了基于Flow Matching的3D生成模型,在ShapeNetCore-V2数据集上进行了类条件生成,在约60万个形状-文本对上进行了文本到3D的生成,取得了很好的效果。&lt;/li&gt;
  &lt;li&gt;与其他常见的3D表示方法相比,M-SDF在近似精度、计算效率、参数效率等方面具有优势。&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html"></summary></entry><entry><title type="html">A Dataset and Explorer for 3D Signed Distance Functions</title><link href="https://ucas.io/ai/3d/SDF-McGuire/" rel="alternate" type="text/html" title="A Dataset and Explorer for 3D Signed Distance Functions" /><published>2024-04-09T20:16:01+08:00</published><updated>2024-04-09T20:16:01+08:00</updated><id>https://ucas.io/ai/3d/SDF-McGuire</id><content type="html" xml:base="https://ucas.io/ai/3d/SDF-McGuire/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240409201627.png&quot; alt=&quot;20240409201627&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-dataset-and-explorer-for-3d-signed-distance-functions&quot;&gt;A Dataset and Explorer for 3D Signed Distance Functions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;主要贡献
    &lt;ul&gt;
      &lt;li&gt;一个python和c++结合的shader&lt;a href=&quot;https://github.com/tovacinni/sdf-explorer/tree/master&quot;&gt;渲染器&lt;/a&gt;，包含glsl&lt;/li&gt;
      &lt;li&gt;讨论了sdf的好处
        &lt;ul&gt;
          &lt;li&gt;其中讨论了Eikonal equation和Lipschitz constant&lt;/li&gt;
          &lt;li&gt;讨论了一些sdf的CSG操作&lt;/li&gt;
          &lt;li&gt;讨论了一下sdf在ray-matching上的好处&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;主要整理了一下几个基于sdf的数据集&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html"></summary></entry><entry><title type="html">Dalle3</title><link href="https://ucas.io/ai/3d/Dit/" rel="alternate" type="text/html" title="Dalle3" /><published>2024-03-29T15:03:34+08:00</published><updated>2024-03-29T15:03:34+08:00</updated><id>https://ucas.io/ai/3d/Dit</id><content type="html" xml:base="https://ucas.io/ai/3d/Dit/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240329150229.png&quot; alt=&quot;20240329142726&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dit&quot;&gt;Dit&lt;/h2&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html"></summary></entry><entry><title type="html">VQ-VAE</title><link href="https://ucas.io/ai/3d/VQ-VAE/" rel="alternate" type="text/html" title="VQ-VAE" /><published>2024-03-26T22:12:34+08:00</published><updated>2024-03-26T22:12:34+08:00</updated><id>https://ucas.io/ai/3d/VQ-VAE</id><content type="html" xml:base="https://ucas.io/ai/3d/VQ-VAE/">&lt;h1 id=&quot;vq-vae&quot;&gt;VQ-VAE&lt;/h1&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Stage-1 (latent space learning)&lt;/th&gt;
      &lt;th&gt;Latent Space&lt;/th&gt;
      &lt;th&gt;Stage-2 (prior learning)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;VQ-VAE&lt;/td&gt;
      &lt;td&gt;VQ-VAE&lt;/td&gt;
      &lt;td&gt;Discrete (after quantization)&lt;/td&gt;
      &lt;td&gt;Autoregressive PixelCNN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VQGAN&lt;/td&gt;
      &lt;td&gt;VQGAN (VQ-VAE + GAN + Perceptual Loss)&lt;/td&gt;
      &lt;td&gt;Discrete (after quantization)&lt;/td&gt;
      &lt;td&gt;Autoregressive GPT-2 (Transformer)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VQ-Diffusion&lt;/td&gt;
      &lt;td&gt;VQ-VAE&lt;/td&gt;
      &lt;td&gt;Discrete (after quantization)&lt;/td&gt;
      &lt;td&gt;Discrete Diffusion&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Latent Diffusion (VQ-reg)&lt;/td&gt;
      &lt;td&gt;VAE or VQGAN&lt;/td&gt;
      &lt;td&gt;Continuous (before quantization)&lt;/td&gt;
      &lt;td&gt;Continuous Diffusion&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240326221537.png&quot; alt=&quot;20240209194938&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AE学习得到的隐空间并不规整
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;AE(Autoencoder,自编码器)是一种无监督学习模型,由编码器(Encoder)和解码器(Decoder)组成。编码器将输入数据映射到一个通常维度更低的隐空间(latent space),解码器则从隐空间重构出原始输入。AE的目标是最小化重构误差,即输入和重构输出之间的差异。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;当我们说AE的编码器编码出来的向量空间是不规整的,主要是指以下几个方面:&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;空间分布不均匀:隐空间中的数据点分布可能不均匀,某些区域的数据点密度可能很高,而其他区域的数据点稀疏。这可能导致生成的样本质量不稳定。&lt;/li&gt;
      &lt;li&gt;空间缺乏连续性:相似的输入数据点在隐空间中可能不会映射到相近的位置,空间的连续性不够平滑。这意味着在隐空间中进行插值或采样可能生成质量较差的样本。&lt;/li&gt;
      &lt;li&gt;空间缺乏解释性:隐空间中的维度可能没有明确的语义解释,不同维度的变化对输入数据的影响难以解释。这限制了我们对隐空间的理解和控制。&lt;/li&gt;
      &lt;li&gt;空间缺乏结构性:隐空间可能没有明显的层次结构或规律性,不同类别的数据在隐空间中可能混杂在一起,缺乏清晰的分类边界。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vq-vae-目标&quot;&gt;VQ-VAE 目标&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;编码器(Encoder):将输入数据x映射到一个连续的隐空间,得到隐变量z。与普通VAE不同的是,VQ-VAE希望z是离散的。&lt;/li&gt;
  &lt;li&gt;矢量量化(Vector Quantization):引入一个离散的码本e,它包含K个D维的码字向量ek。隐变量z通过与码本中每个码字计算L2距离,找到最近的码字ek,并用其替代原始的z。这个量化过程使得隐变量离散化。&lt;/li&gt;
  &lt;li&gt;解码器(Decoder):将量化后的隐变量ek作为输入,重构出原始数据x。&lt;/li&gt;
  &lt;li&gt;训练目标:VQ-VAE优化三个损失函数: (1)重构损失:衡量重构数据与原始数据的差异; (2)码字向量关于隐变量z的损失:使编码器产生的隐变量接近码本中的码字向量; (3)码本的损失:通过优化码本,使码字向量能够捕捉到数据的局部特征。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;缺点&quot;&gt;缺点&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;量化操作不可导:在训练过程中,VQ-VAE的量化操作是不可导的,这使得端到端的梯度反向传播变得困难。通常需要使用一些梯度估计技巧如Straight-Through Estimator来绕开这个问题,但这可能影响模型的优化和收敛速度。&lt;/li&gt;
  &lt;li&gt;码本大小的选择:VQ-VAE中码本的大小K是一个需要提前设定的超参数。K的选择会影响模型的表示能力和计算复杂度。K太小,码本可能无法充分捕捉数据的多样性;K太大,则会增加计算开销,并可能导致某些码字向量无法得到有效利用。&lt;/li&gt;
  &lt;li&gt;码字利用率不平衡:在训练过程中,某些码字向量可能被频繁使用,而另一些却很少被用到。这导致码本的利用率不平衡,某些码字向量可能冗余。需要引入额外的机制如码本损失或指数移动平均来缓解这个问题。&lt;/li&gt;
  &lt;li&gt;隐变量的独立性假设:VQ-VAE假设隐变量之间是独立的,忽略了它们之间可能存在的结构或关系。这限制了模型捕捉某些复杂数据的能力。一些后续工作如VQ-VAE-2尝试通过引入分层结构来缓解这个问题。&lt;/li&gt;
  &lt;li&gt;生成多样性不足:相比一些其他生成模型如GAN,VQ-VAE的生成结果可能缺乏多样性和细节。这可能是因为离散隐变量的表示能力有限,以及解码器网络结构的限制。&lt;/li&gt;
  &lt;li&gt;适用范围有限:VQ-VAE更适用于那些需要离散特征表示的任务,如语音识别、语言建模等。对于一些需要连续潜在表示的任务,普通的VAE可能更合适。&lt;/li&gt;
  &lt;li&gt;Perplexity值越高,说明模型使用的unique codebook向量越多,codebook的利用率就越高。这通常意味着模型能够从数据中学习到更丰富的表示。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vq-vae-2&quot;&gt;VQ-VAE-2&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240328120859.png&quot; alt=&quot;20240328120859&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我们知道自回归模型的一大缺点是生成速度慢，这是因为一张高清图像的像素成千上万，而自回归模型只能串行生成一个个像素。为了解决这个问题，我们可以把自回归模型放到隐空间去，这样不仅速度成倍加快，信息密度也更高——像素空间中冗余繁杂的细节信息被压缩掉了，模型只需要考虑真正重要的语义信息。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;整体架构：
    &lt;ul&gt;
      &lt;li&gt;第一阶段是训练层次化的VQ-VAE编码器和解码器:
        &lt;ul&gt;
          &lt;li&gt;编码器由多个下采样块组成,将输入图像逐步编码为离散的潜码。第一个编码器先将图像编码为较低层次的潜码(如 64x64),然后第二个编码器在此基础上继续编码为更高层次的潜码(如32x32)。不同层次的编码器通过残差连接组合在一起。&lt;/li&gt;
          &lt;li&gt;解码器采用与编码器相反的结构,由多个上采样块组成。它以多个层次的离散潜码为输入,通过解码和上采样,逐步重建出原始图像。&lt;/li&gt;
          &lt;li&gt;整个VQ-VAE的训练目标是最小化重建误差,同时使用一些正则项如codebook loss和commitment loss来优化离散潜码的学习。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;第二阶段是在学习到的离散潜码空间上训练强大的自回归先验模型:
        &lt;ul&gt;
          &lt;li&gt;先将所有图像通过训练好的VQ-VAE编码器编码为离散潜码,构建新的训练集。&lt;/li&gt;
          &lt;li&gt;在低层次的潜码(如64x64)上训练一个conditional PixelCNN先验模型,将高层次潜码(如32x32)作为条件输入。&lt;/li&gt;
          &lt;li&gt;在高层次潜码上训练一个无条件的PixelCNN先验模型。为了捕捉长距离依赖,该先验模型中使用了self-attention层。&lt;/li&gt;
          &lt;li&gt;最终的图像生成过程为:先通过高层次的PixelCNN采样高层潜码,再将其作为条件输入到底层PixelCNN中采样底层潜码。然后将采样得到的多个层次的潜码输入到VQ-VAE解码器中解码,得到最终的图像。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;优点:
    &lt;ul&gt;
      &lt;li&gt;通过使用分层的离散潜变量表示,VQ-VAE-2可以生成高质量、高分辨率的图像样本,其效果可以与最先进的GAN模型相媲美。&lt;/li&gt;
      &lt;li&gt;VQ-VAE-2使用简单的前馈编码器和解码器网络,编码和解码速度很快。相比直接在像素空间采样,VQ-VAE-2只需要在压缩后的潜码空间进行自回归采样,速度提高了一个数量级。&lt;/li&gt;
      &lt;li&gt;VQ-VAE-2可以很好地捕捉数据分布的所有模式,生成样本的多样性优于GAN。不会出现GAN常见的模式坍塌问题。&lt;/li&gt;
      &lt;li&gt;不同于GAN难以评估,VQ-VAE-2作为一个似然性模型,可以通过测试集上的似然性来客观评估泛化能力,监控过拟合情况。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;缺点:
    &lt;ul&gt;
      &lt;li&gt;VQ-VAE-2的重建图像会丢失一些高频细节信息,因此FID等指标比实际感知质量要差一些。&lt;/li&gt;
      &lt;li&gt;尽管VQ-VAE-2的采样速度比像素空间快很多,但是其自回归先验模型的训练仍然是个瓶颈,在大规模高分辨率数据集上训练需要巨大的计算开销。&lt;/li&gt;
      &lt;li&gt;VQ-VAE-2涉及两个阶段的训练,先训练VQ-VAE再训练先验模型,流程稍显复杂。端到端训练可能会进一步提升性能。&lt;/li&gt;
      &lt;li&gt;为了提高样本质量,本文还提出了基于分类器的拒绝采样等附加技巧,一定程度上削弱了似然性模型的优势。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;QA
    &lt;ul&gt;
      &lt;li&gt;为什么要用自回归函数
        &lt;ul&gt;
          &lt;li&gt;自回归函数可以建模更长距离的依赖关系。如果只依赖周围8个像素,CNN的感受野很小,只能捕捉到局部的相关性。而图像中存在很多长程依赖,如物体的整体结构、重复的纹理等。自回归模型可以通过深层网络和扩大感受野来建模这些长程依赖。&lt;/li&gt;
          &lt;li&gt;自回归函数可以显式地估计数据分布。给定之前像素的条件下,自回归模型可以预测下一个像素的概率分布。这意味着我们可以显式地计算整个图像的似然概率,从而可以用最大似然估计等方法来训练模型,更加稳定和可控。&lt;/li&gt;
          &lt;li&gt;自回归模型可以实现无条件和条件生成。无条件生成时,我们可以从空白图像开始,逐像素地采样生成新图像。条件生成时,我们可以固定一部分像素,对剩余像素进行采样,实现图像补全、修复等任务。这种灵活性是简单CNN不具备的。&lt;/li&gt;
          &lt;li&gt;自回归模型可以捕捉数据中的多模态性。对于一个给定的像素上下文,下一个像素的分布可能是多峰的,即存在多种可能的取值。自回归模型可以通过估计多峰概率分布来反映这种多模态性,而简单CNN通常只能预测单一的值。&lt;/li&gt;
          &lt;li&gt;自回归模型可以结合其他先验知识。我们可以在自回归模型中加入一些先验假设,如平移不变性、旋转不变性等,进一步提高模型的表示能力。这可以通过在网络中加入卷积层、池化层等结构来实现。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vq-gan&quot;&gt;VQ-GAN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img1711610588920.jpg&quot; alt=&quot;1711610588920&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;任务&lt;/th&gt;
      &lt;th&gt;数据集&lt;/th&gt;
      &lt;th&gt;图片分辨率&lt;/th&gt;
      &lt;th&gt;Codebook大小&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;无条件图像生成&lt;/td&gt;
      &lt;td&gt;ImageNet&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;无条件图像生成&lt;/td&gt;
      &lt;td&gt;FacesHQ, FFHQ&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;16384&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;无条件图像生成&lt;/td&gt;
      &lt;td&gt;LSUN Churches &amp;amp; Towers&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;语义图像合成&lt;/td&gt;
      &lt;td&gt;ADE20K&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;4096&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;语义图像合成&lt;/td&gt;
      &lt;td&gt;COCO-Stuff&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;8192&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;语义图像合成&lt;/td&gt;
      &lt;td&gt;S-FLCKR (Landscapes)&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;语义图像合成&lt;/td&gt;
      &lt;td&gt;S-FLCKR (Landscapes)&lt;/td&gt;
      &lt;td&gt;1024×1024&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;深度图像合成&lt;/td&gt;
      &lt;td&gt;RIN (Restricted ImageNet)&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;边缘图像合成&lt;/td&gt;
      &lt;td&gt;ImageNet&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;姿态引导合成&lt;/td&gt;
      &lt;td&gt;DeepFashion&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Class-conditional生成&lt;/td&gt;
      &lt;td&gt;ImageNet&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;16384&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Class-conditional生成&lt;/td&gt;
      &lt;td&gt;RIN (Restricted ImageNet)&lt;/td&gt;
      &lt;td&gt;256×256&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;主体
    &lt;ul&gt;
      &lt;li&gt;提出了一种两阶段的图像合成方法,先用卷积网络VQ-GAN学习图像的紧凑表示(codebook),然后用Transformer对codebook进行建模,从而生成图像。这种方法可以显著降低Transformer处理的序列长度。&lt;/li&gt;
      &lt;li&gt;VQ-GAN引入了感知损失和对抗训练,可以在高压缩率下得到高保真的重建图像。这为Transformer提供了信息丰富的视觉词汇。&lt;/li&gt;
      &lt;li&gt;在固定计算资源下,Transformer可以比卷积网络PixelCNN在学习到的离散表示上取得更好的建模效果。
所提出的方法可以用于多种条件图像合成任务,包括从语义分割图、深度图、边缘图、姿态等不同输入生成图像。&lt;/li&gt;
      &lt;li&gt;通过滑动窗口的方式,该方法可以生成高达1024×1024分辨率的图像。在人脸、教堂等数据集上可以生成高保真逼真的图像样本。&lt;/li&gt;
      &lt;li&gt;在ImageNet 256×256的class-conditional图像生成任务上,该方法优于之前的autoregressive模型如VQ-VAE-2,且推断速度更快。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;QA
    &lt;ul&gt;
      &lt;li&gt;为什么codebook是紧凑表示
        &lt;ul&gt;
          &lt;li&gt;codebook中的每个entry(通常称为code)是一个固定长度的向量,用来表示图像的一个局部区域(patch)。相比原始像素空间,用离散的code表示图像可以显著降低信息冗余。&lt;/li&gt;
          &lt;li&gt;论文使用VQ-GAN以自监督的方式学习codebook。训练过程会最小化原始图像和重建图像(通过code解码得到)的感知差异。因此,学到的code能够尽可能保留原始图像的语义信息。&lt;/li&gt;
          &lt;li&gt;codebook的大小(即code的数量)远小于图像像素的数量。论文中使用的codebook大小在512到16384之间,而高分辨率图像的像素数量可达百万级。&lt;/li&gt;
          &lt;li&gt;通过在Transformer中建模code的空间排布,而不是直接建模像素,可以将建模对象的数量从百万级降低到成千上万。这大大提高了建模高分辨率图像的计算效率。&lt;/li&gt;
          &lt;li&gt;实验表明,合适大小的codebook可以在信息压缩和重建质量之间取得很好的平衡。过小的codebook会导致重建失真,过大则会降低计算效率。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vq-diffusion&quot;&gt;VQ-Diffusion&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img1711612934215.jpg&quot; alt=&quot;1711612934215&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;整体
    &lt;ul&gt;
      &lt;li&gt;先用VQ-VAE将图像压缩编码到离散的latent空间中,从而大大减少了图像的序列长度。&lt;/li&gt;
      &lt;li&gt;在离散的latent空间上应用conditional diffusion模型来建模latent codes的分布。通过逆向扩散过程,可以从纯噪声逐步去噪还原出符合文本描述的图像latent codes。&lt;/li&gt;
      &lt;li&gt;提出了一种mask-and-replace的diffusion策略,在每个去噪步骤中引入masked token,避免了生成误差的累积。&lt;/li&gt;
      &lt;li&gt;模型去除了自回归(auto-regressive)方法中存在的单向性偏差,同时生成速度更快&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;什么是masked token
    &lt;ul&gt;
      &lt;li&gt;在编码图像为离散tokens时,除了原有的visual token,还引入了一种特殊的[MASK] token。
在diffusion的前向过程中,每个visual token有一定概率γ被替换成[MASK]。同时有一个较小的概率β被随机替换成其他visual token。剩下的概率保持原始token不变。&lt;/li&gt;
      &lt;li&gt;模型训练时并不使用teacher forcing,而是刻意引入mask和随机替换的token,让模型学习在corruption较重的情况下也能预测出原始的visual token。&lt;/li&gt;
      &lt;li&gt;在逆向去噪过程中,[MASK] token起到显式指示噪声最重区域的作用。模型将更多地关注被mask的区域,在每一步根据上下文信息去预测[MASK]处原本的visual token,从而使去噪更加高效。&lt;/li&gt;
      &lt;li&gt;随机替换是为了避免模型过于关注[MASK],同时强迫模型去理解更广泛的上下文。而保留部分原始token则可以为恢复提供重要的先验信息。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;这个机制是作者第一次使用的吗，还是从其他论文中借鉴出来的
    &lt;ul&gt;
      &lt;li&gt;Mask-and-replace这个机制并非作者首创,而是借鉴了其他领域的思想:
        &lt;ul&gt;
          &lt;li&gt;Masked language modeling(MLM): 这是近年来NLP领域非常流行的一种预训练范式,最早由BERT模型提出。MLM通过随机mask掉一部分输入token,训练模型根据上下文预测被mask掉的token,从而让模型学习到良好的语言表征。VQ-Diffusion中的mask思想可以看作是将MLM思想迁移到了视觉领域。&lt;/li&gt;
          &lt;li&gt;Diffusion model: Diffusion过程最初由Sohl-Dickstein等人于2015年提出,近年来在CV领域受到越来越多的关注。传统的diffusion过程中仅仅使用了token replacement,VQ-Diffusion将其与mask策略相结合,是一个新颖的尝试。&lt;/li&gt;
          &lt;li&gt;其他工作: 之前也有一些将diffusion应用于离散数据的尝试,如Argmax Flow和D3PM,但它们主要关注文本生成或是低分辨率像素空间上的建模。将discrete diffusion应用于图像token空间并引入mask机制,VQ-Diffusion是最早的工作之一。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/633744455&quot;&gt;轻松理解 VQ-VAE：首个提出 codebook 机制的生成模型 - 周弈帆的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/619398226&quot;&gt;Vector-Quantized VAE / GAN / Diffusion - xyfJASON的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zalandoresearch/pytorch-vq-vae&quot;&gt;PyTorch implementation of VQ-VAE by Aäron van den Oord et al. - github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">VQ-VAE</summary></entry><entry><title type="html">Dalle3</title><link href="https://ucas.io/ai/3d/Dalle3/" rel="alternate" type="text/html" title="Dalle3" /><published>2024-03-26T22:12:34+08:00</published><updated>2024-03-26T22:12:34+08:00</updated><id>https://ucas.io/ai/3d/Dalle3</id><content type="html" xml:base="https://ucas.io/ai/3d/Dalle3/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240329142726.png&quot; alt=&quot;20240329142726&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;dalle3&quot;&gt;Dalle3&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cdn.openai.com/papers/dall-e-3.pdf&quot;&gt;Improving Image Generation with Better Captions - dalle3&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;关于生成模型
        &lt;ul&gt;
          &lt;li&gt;基础模型:先在图文对数据集上预训练了一个类似于语言模型的图像描述生成器。输入为图像的CLIP编码和已生成的字符,输出为下一个字符的概率分布。这样模型就学会了根据图像内容生成与之匹配的描述文本。&lt;/li&gt;
          &lt;li&gt;短句微调:在此基础上,作者又准备了一个小规模的数据集,其中的描述只关注图像的主要内容。用这个数据集微调上一步的模型,使其倾向于生成简洁的描述文本。这一步得到的模型被称为”short synthetic captions”。&lt;/li&gt;
          &lt;li&gt;长句微调:类似地,作者再准备一个小规模的长描述数据集,其中的描述不仅涉及图像主体,还包含了背景、数量、颜色等丰富的细节。再次用这个数据集微调,得到的模型称为”descriptive synthetic captions”,可以生成详尽的描述。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;关于提高文本到图像生成系统能力
        &lt;ul&gt;
          &lt;li&gt;现有的文本到图像模型在跟随详细的图像描述指令方面存在困难,经常会忽略或混淆文字中的含义。&lt;strong&gt;作者认为这个问题源自训练数据中的图像描述文本质量较差,存在噪声和不准确性&lt;/strong&gt;。&lt;/li&gt;
          &lt;li&gt;为了解决这个问题,作者训练了一个定制的图像描述生成模型,用它重新为训练图像生成了更详细、准确的人工描述文本。然后用这些高质量的合成描述来训练文本到图像模型。&lt;/li&gt;
          &lt;li&gt;实验结果表明,用合成的描述训练可以显著提高模型对输入文本的理解和表达能力。基于这个发现,作者构建了一个新的系统DALL-E 3。&lt;/li&gt;
          &lt;li&gt;在对DALL-E 3进行评估时,考察了它在理解指令、生成连贯图像、图像美感等方面的表现,结果优于目前的一些竞争模型。作者还公开了评估所用的样本和代码,以推动后续研究进一步优化文本到图像模型的这些重要能力。&lt;/li&gt;
          &lt;li&gt;论文还讨论了当前方法的局限性,例如DALL-E 3在空间感知、生成文本、理解特定概念等方面仍有不足,未来可通过改进图像描述生成器等方式来提升。同时还分析了系统可能带来的风险,并在附录中详细阐述了安全性和偏差的缓解措施。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;使用的文本编辑器
        &lt;ul&gt;
          &lt;li&gt;T5 (Text-to-Text Transfer Transformer)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;即过度拟合数据集中的分布规律。
        &lt;ul&gt;
          &lt;li&gt;
            &lt;blockquote&gt;
              &lt;p&gt;Likelihood models like our text-to-image diffusion models have a notorious tendency to overfit to distributional regularities in the dataset. For example, a text-to-image model that is trained on text that always starts with a space character will not work properly if you try to perform inference with prompts that do not also start with     that space.&lt;/p&gt;
            &lt;/blockquote&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.11807&quot;&gt;Improving Image Captioning with Better Use of Captions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;目标
    &lt;ul&gt;
      &lt;li&gt;使用高度描述性的生成的caption上训练文生图模型，可以显著提高文本到图像模型的提示跟随能力&lt;/li&gt;
      &lt;li&gt;重点评估了DALL-E 3在高度描述性生成标题的训练下改进的提示跟踪。它不涉及DALL-E 3模型的训练或实现细节。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;这篇论文提出了一个新的图像描述生成框架,主要有以下几个创新点:
    &lt;ul&gt;
      &lt;li&gt;利用图像描述文本构建可视化关系图(Caption-Guided Visual Relationship Graph, CGVRG)。之前的方法使用预训练的视觉关系检测模型构建关系图,但这些模型常忽略一些对描述任务很重要的关系。本文使用图像描述文本提取三元组(主体-谓词-宾语),然后用弱监督的多示例学习方法将三元组中的谓词与图像中的区域配对,以此构建面向描述任务的可视化关系图。&lt;/li&gt;
      &lt;li&gt;通过图卷积操作,将CGVRG中节点的视觉和文本特征融合,增强节点表示能力。
        &lt;ul&gt;
          &lt;li&gt;图卷积&lt;span class=&quot;kdmath&quot;&gt;$h_i^{(l+1)} = \sigma\left(\sum_{j\in \mathcal{N}(i)} \frac{1}{c_{ij}} W^{(l)} h_j^{(l)}\right)$&lt;/span&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;在描述生成阶段,提出了一个多任务学习模块。模型在生成每个单词的同时,还预测该单词是主体、谓词还是其他。这种方式可以帮助模型判断在生成每个单词时,应该着重参考CGVRG中的哪类节点信息。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/682703025&quot;&gt;Dalle-3论文阅读 - nlpcver的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html"></summary></entry><entry><title type="html">OpenClip</title><link href="https://ucas.io/ai/3d/OpenClip/" rel="alternate" type="text/html" title="OpenClip" /><published>2024-03-26T14:29:40+08:00</published><updated>2024-03-26T14:29:40+08:00</updated><id>https://ucas.io/ai/3d/OpenClip</id><content type="html" xml:base="https://ucas.io/ai/3d/OpenClip/">&lt;h1 id=&quot;openclip-对齐文本和图像的隐空间&quot;&gt;OpenClip, 对齐文本和图像的隐空间&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png&quot; alt=&quot;20240209194938&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.04725&quot;&gt;OpenClip&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;对比学习
        &lt;ul&gt;
          &lt;li&gt;对比学习(Contrastive Learning)是一种自监督学习方法,旨在学习数据的通用表示,以便在下游任务中可以更好地利用这些表示。其核心思想是通过最大化相似样本对之间的相似度,同时最小化不相似样本对之间的相似度,来学习数据的特征表示。具体而言:&lt;/li&gt;
          &lt;li&gt;正样本对:对同一实例的两个随机增强版本(如图像的裁剪、翻转、颜色变化等)构成正样本对,模型需要学习将它们的特征表示拉近。&lt;/li&gt;
          &lt;li&gt;负样本对:将不同实例的特征表示作为负样本对,模型需要将它们的特征表示拉远。&lt;/li&gt;
          &lt;li&gt;对比损失:通过一个对比损失函数(如InfoNCE损失)来优化上述目标,使相似样本对的特征表示相近,不相似样本对的特征表示相异。
            &lt;ul&gt;
              &lt;li&gt;常见的对比损失函数
                &lt;ul&gt;
                  &lt;li&gt;InfoNCE损失(Information Noise-Contrastive Estimation): InfoNCE损失源自NCE (Noise-Contrastive Estimation),是一种常用的对比损失函数。对于一个正样本对(i,j),InfoNCE损失定义为: $L_{i,j} = -\log \frac{\exp(sim(z_i,z_j)/\tau)}{\sum_{k=1}^N \exp(sim(z_i,z_k)/\tau)}$ 其中,$z_i$和$z_j$是正样本对的特征表示,$z_k$是负样本的特征表示,$sim$是相似度度量(如点积),$\tau$是温度超参数,N是负样本的数量。&lt;/li&gt;
                  &lt;li&gt;Triplet损失(Triplet Loss): Triplet损失通过构建三元组(anchor, positive, negative)来优化样本间的相对距离。其目标是anchor与positive之间的距离小于anchor与negative之间的距离。数学表达为: $L = \max(d(a,p) - d(a,n) + \mathit{margin}, 0)$ 其中,$d$是距离度量(如欧氏距离),$\mathit{margin}$是一个正的边界值。&lt;/li&gt;
                  &lt;li&gt;NT-Xent损失(Normalized Temperature-scaled Cross Entropy Loss): NT-Xent损失是SimCLR中使用的损失函数,类似于InfoNCE损失,但进行了标准化和对称化处理。对于一对正样本(i,j),NT-Xent损失定义为: $l_{i,j} = -\log \frac{\exp(sim(z_i,z_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}{[k \neq i]} \exp(sim(z_i,z_k)/\tau)}$ 其中,$\mathbf{1}{[k \neq i]} \in {0, 1}$是指示函数,N是批次大小。NT-Xent损失在批次内对所有样本对进行计算,并进行对称化处理。&lt;/li&gt;
                  &lt;li&gt;Contrastive Cross Entropy损失: 对比交叉熵损失将对比学习问题看作是二分类问题,正样本对应标签1,负样本对应标签0。损失函数定义为: $L = -\frac{1}{N} \sum_{i=1}^N \left( y_i \cdot \log(\hat{y}_i) + (1-y_i) \cdot \log(1-\hat{y}_i) \right)$ 其中,$y_i$是样本对的真实标签,$\hat{y}_i$是模型预测的相似度。&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;对比学习可以应用于各种类型的数据,如图像、文本、语音等。一些著名的对比学习方法包括:
            &lt;ul&gt;
              &lt;li&gt;SimCLR (Simple Framework for Contrastive Learning of Visual Representations)&lt;/li&gt;
              &lt;li&gt;MoCo (Momentum Contrast for Unsupervised Visual Representation Learning)&lt;/li&gt;
              &lt;li&gt;CLIP (Contrastive Language-Image Pre-training)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;在CLIP中,通过对比学习,模型可以学习到图像和文本的统一表示空间,在此空间中语义相似的图像和文本更加靠近。这种对比学习方式使得CLIP模型具有良好的零样本和少样本学习能力。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;多模态embedding
        &lt;ul&gt;
          &lt;li&gt;如图所示，给定一个 Batch 的 N 个 (图片，文本) 对，图片输入给 Image Encoder 得到表征 I1, I2, …, IN ，文本输入给Text Encoder得到表征 T1, T2, …, TN, 然后通过“对比学习”，找到图像和文字的相似关系。对于图中列出的N*N个格子，只需计算每个格子上对应的向量点积（余弦相似度）。由于对角线上的图片-文字对是真值，希望对角线上的相似度可以最大，据此可设置交叉熵函数，来求得每个batch下的Loss。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;预训练
        &lt;ul&gt;
          &lt;li&gt;视觉预训练
            &lt;ul&gt;
              &lt;li&gt;通过对比学习使得同一张图像的不同裁剪或变换间距离更近，不同图像间距离更远。这样模型就能学习有区分度的视觉特征表示&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;视觉-语言预训练
            &lt;ul&gt;
              &lt;li&gt;对比学习的方法，使得相同含义的不同图像和文本之间的距离更近，而不同含义的图像和文本之间的距离更远&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Zero Shot预测
        &lt;ul&gt;
          &lt;li&gt;首先，创建一个标签全集，如图中（2）所示，并得到每一个标签的特征向量&lt;/li&gt;
          &lt;li&gt;然后，取一张图片，如图中（3）所示，过Image Encoder后得到该图片的特征向量&lt;/li&gt;
          &lt;li&gt;最后，计算图片向量和文字向量间的相似度，取相似度最高的那条label即可。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;优势/劣势
        &lt;ul&gt;
          &lt;li&gt;优势
            &lt;ul&gt;
              &lt;li&gt;无监督或弱监督的学习方法&lt;/li&gt;
              &lt;li&gt;泛化能力强&lt;/li&gt;
              &lt;li&gt;可解释性好&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;劣势
            &lt;ul&gt;
              &lt;li&gt;文字标签是个闭集：模型预测一张新的图像，只能从已有的标签集合中找出最相似的，不能预测一个新标签。&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;数据集
        &lt;ul&gt;
          &lt;li&gt;LAION-5B 数据集，包含 58 亿个密切相关的图像-文本对&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stable-diffusion-对比&quot;&gt;Stable diffusion 对比&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;组件&lt;/th&gt;
      &lt;th&gt;SDXL&lt;/th&gt;
      &lt;th&gt;SD 1.4/1.5&lt;/th&gt;
      &lt;th&gt;SD 2.0/2.1&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;UNet 参数&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;参数量&lt;/td&gt;
      &lt;td&gt;2.6B&lt;/td&gt;
      &lt;td&gt;860M&lt;/td&gt;
      &lt;td&gt;865M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Transformer blocks&lt;/td&gt;
      &lt;td&gt;[0, 2, 10]&lt;/td&gt;
      &lt;td&gt;[1, 1, 1, 1]&lt;/td&gt;
      &lt;td&gt;[1, 1, 1, 1]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Channel mult.&lt;/td&gt;
      &lt;td&gt;[1, 2, 4]&lt;/td&gt;
      &lt;td&gt;[1, 2, 4, 4]&lt;/td&gt;
      &lt;td&gt;[1, 2, 4, 4]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;文本编码器&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;模型&lt;/td&gt;
      &lt;td&gt;CLIP ViT-L &amp;amp; OpenCLIP ViT-bigG&lt;/td&gt;
      &lt;td&gt;CLIP ViT-L&lt;/td&gt;
      &lt;td&gt;OpenCLIP ViT-H&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Context dim.&lt;/td&gt;
      &lt;td&gt;2048&lt;/td&gt;
      &lt;td&gt;768&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pooled text emb.&lt;/td&gt;
      &lt;td&gt;OpenCLIP ViT-bigG&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;VAE&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Reconstruction&lt;/td&gt;
      &lt;td&gt;优于 SD 1.x/2.x&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;训练设置&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;分辨率&lt;/td&gt;
      &lt;td&gt;256-&amp;gt;512-&amp;gt;1024&lt;/td&gt;
      &lt;td&gt;256/512&lt;/td&gt;
      &lt;td&gt;512/768&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Aspect Ratios&lt;/td&gt;
      &lt;td&gt;支持多种&lt;/td&gt;
      &lt;td&gt;正方形&lt;/td&gt;
      &lt;td&gt;正方形&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Conditioning&lt;/td&gt;
      &lt;td&gt;大小,裁剪,长宽比&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Refinement&lt;/td&gt;
      &lt;td&gt;额外的refiner模型&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;openclip-模型对比&quot;&gt;Openclip 模型对比&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model name&lt;/th&gt;
      &lt;th&gt;Batch size&lt;/th&gt;
      &lt;th&gt;Samples seen&lt;/th&gt;
      &lt;th&gt;Text Params&lt;/th&gt;
      &lt;th&gt;Image params&lt;/th&gt;
      &lt;th&gt;ImageNet top1&lt;/th&gt;
      &lt;th&gt;Mscoco image retrieval at 5&lt;/th&gt;
      &lt;th&gt;Flickr30k image retrieval at 5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;OpenAI CLIP L/14&lt;/td&gt;
      &lt;td&gt;32k&lt;/td&gt;
      &lt;td&gt;13B&lt;/td&gt;
      &lt;td&gt;123.65M&lt;/td&gt;
      &lt;td&gt;303.97M&lt;/td&gt;
      &lt;td&gt;75.4%&lt;/td&gt;
      &lt;td&gt;61%&lt;/td&gt;
      &lt;td&gt;87%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OpenCLIP H/14&lt;/td&gt;
      &lt;td&gt;79k&lt;/td&gt;
      &lt;td&gt;32B (16 epochs of laion2B)&lt;/td&gt;
      &lt;td&gt;354.0M&lt;/td&gt;
      &lt;td&gt;632.08M&lt;/td&gt;
      &lt;td&gt;78%&lt;/td&gt;
      &lt;td&gt;73.4%&lt;/td&gt;
      &lt;td&gt;94%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OpenCLIP G/14&lt;/td&gt;
      &lt;td&gt;160k&lt;/td&gt;
      &lt;td&gt;32B +unmasked fine-tune (details below)&lt;/td&gt;
      &lt;td&gt;694.7M&lt;/td&gt;
      &lt;td&gt;1844.9M&lt;/td&gt;
      &lt;td&gt;80.1%*&lt;/td&gt;
      &lt;td&gt;74.9%&lt;/td&gt;
      &lt;td&gt;94.9%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CoCa&lt;/td&gt;
      &lt;td&gt;66k&lt;/td&gt;
      &lt;td&gt;33B&lt;/td&gt;
      &lt;td&gt;1100M&lt;/td&gt;
      &lt;td&gt;1000M&lt;/td&gt;
      &lt;td&gt;86.3%**&lt;/td&gt;
      &lt;td&gt;74.2%&lt;/td&gt;
      &lt;td&gt;95.7%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;OpenCLIP G/14模型的ImageNet top1精度有个星号,说明该数值可能有些特殊情况。
CoCa模型的ImageNet top1精度有两个星号,也表示该数值的测试条件可能与其他模型不完全一致。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/xs1997/article/details/134650192&quot;&gt;AIGC系列之：CLIP和OpenCLIP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mdnice.com/writing/682896d9c0e74709a46d85da4fbfecaa&quot;&gt;Stable Diffusion XL(SDXL)原理详解&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">OpenClip, 对齐文本和图像的隐空间</summary></entry><entry><title type="html">Sora: The paper you need to read</title><link href="https://ucas.io/ai/3d/Sora/" rel="alternate" type="text/html" title="Sora: The paper you need to read" /><published>2024-02-19T15:37:48+08:00</published><updated>2024-02-19T15:37:48+08:00</updated><id>https://ucas.io/ai/3d/Sora</id><content type="html" xml:base="https://ucas.io/ai/3d/Sora/">&lt;h2 id=&quot;papers&quot;&gt;Papers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dall-e 3
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.11807&quot;&gt;Improving Image Captioning with Better Use of Captions
&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://cdn.openai.com/papers/dall-e-3.pdf&quot;&gt;Improving Image Generation with Better Captions - dalle3&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/682703025&quot;&gt;Dalle-3论文阅读 - nlpcver的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vq-vae
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.00937&quot;&gt;Neural Discrete Representation Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DiT
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.09748&quot;&gt;Scalable Diffusion Models with Transformers&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.03429&quot;&gt;Generating Long Videos of Dynamic Scenes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.10157&quot;&gt;VideoGPT: Video Generation using VQ-VAE and Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;&gt;Imagen Video: High Definition Video Generation with Diffusion Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.08818&quot;&gt;Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.06662&quot;&gt;Photorealistic Video Generation with Diffusion Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;&gt;ViViT: A Video Vision Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;长上下文训练
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.08268&quot;&gt;World Model on Million-Length Video And Language With RingAttention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.14509&quot;&gt;DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.12945&quot;&gt;Lumiere: A Space-Time Diffusion Model for Video Generation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lucidrains/magvit2-pytorch&quot;&gt;magvit2&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;video vae encoder-decoder&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.06304&quot;&gt;Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.10789&quot;&gt;Scaling Autoregressive Models for Content-Rich Text-to-Image Generation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.12602&quot;&gt;VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.14125&quot;&gt;VideoPoet: A Large Language Model for Zero-Shot Video Generation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;分辨率上不去&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">Papers</summary></entry></feed>